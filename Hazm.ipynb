{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYOnQR6yXf1jp6csOXR+ZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baharkarami/Text-Mining-Class/blob/main/Hazm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXPTOWAxojV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a21a37e8-8301-434f-e547-e717a480cbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.9.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.5.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.1.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NIxxsKE0CLna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing Hazm: The Hazm library provides various tools for processing Persian text.\n",
        "\n",
        "2. Creating a Normalizer object: An instance of the Normalizer class is created. This class is used to standardize and correct characters in Persian text (e.g., removing extra non-breaking spaces, converting characters to their correct forms, etc.).\n",
        "\n",
        "3. Normalizing the text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uQSgJOLjCMCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "\n",
        "normalizer = Normalizer()\n",
        "normalizer.normalize('قیمت های گزارش شده در سایت های مختلف امروز 11 مرداد به قرار زیر می باشد!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sLZNSteXooqn",
        "outputId": "d625196c-c45e-439e-bfe8-4352061f047e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'قیمت\\u200cهای گزارش\\u200cشده در سایت\\u200cهای مختلف امروز ۱۱ مرداد به قرار زیر می\\u200cباشد!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KASCUd4QCMik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing Hazm.\n",
        "\n",
        "2. Creating a WordTokenizer object.\n",
        "\n",
        "3. Defining the sample text.\n",
        "\n",
        "4. Tokenizing the text:\n",
        "\n",
        "  The splits the input text into a list of words and symbols. The output will include each word or symbol as a separate element in the list.\n"
      ],
      "metadata": {
        "id": "2FdtkhtcCNGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "\n",
        "wrd_tokenizer = WordTokenizer()\n",
        "txt = \"این یک تست است! این ایمیل است b.example@email.com! این هم یک عدد است44!\"\n",
        "wrd_tokenizer.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM_hjvn1p5TW",
        "outputId": "4a6a193d-b328-4315-fe3c-e6a30eefccc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['این',\n",
              " 'یک',\n",
              " 'تست',\n",
              " 'است',\n",
              " '!',\n",
              " 'این',\n",
              " 'ایمیل',\n",
              " 'است',\n",
              " 'b',\n",
              " '.',\n",
              " 'example@email',\n",
              " '.',\n",
              " 'com',\n",
              " '!',\n",
              " 'این',\n",
              " 'هم',\n",
              " 'یک',\n",
              " 'عدد',\n",
              " 'است',\n",
              " '44',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bQrsZaVEKNhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing Hazm.\n",
        "\n",
        "2. Creating a WordTokenizer with replacement settings:\n",
        "\n",
        "  `replace_emails=True`: Replaces email addresses in the text with `[EMAIL]`.\n",
        "\n",
        "  replace_numbers=True: Replaces numbers in the text with `[NUM]`.\n",
        "\n",
        "3. Defining the sample text.\n",
        "\n",
        "4. Tokenizing the text:\n",
        "\n",
        "  This function splits the text into tokens (words and symbols) and replaces emails and numbers as specified."
      ],
      "metadata": {
        "id": "kVqwNG0XKOBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "wrd_tokenizer = hazm.WordTokenizer(replace_emails=True, replace_numbers=True)\n",
        "txt = \"این یک تست است! این ایمیل است b.example@email.com! 44 این هم یک عدد است !\"\n",
        "wrd_tokenizer.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Emi92KtCYz",
        "outputId": "6c44efc1-2a5a-4862-d95d-98bfe3f84e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['این',\n",
              " 'یک',\n",
              " 'تست',\n",
              " 'است',\n",
              " '!',\n",
              " 'این',\n",
              " 'ایمیل',\n",
              " 'است',\n",
              " 'EMAIL',\n",
              " '!',\n",
              " 'NUM',\n",
              " '2',\n",
              " 'این',\n",
              " 'هم',\n",
              " 'یک',\n",
              " 'عدد',\n",
              " 'است',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vOMmXaRCOFAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing the Hazm library:\n",
        "\n",
        "2. Creating an InformalNormalizer object:\n",
        "\n",
        "  An instance of the `InformalNormalizer` class is created. This class is used to normalize colloquial text and convert informal words into more formal ones.\n",
        "\n",
        "3. Defining the input text:\n",
        "\n",
        "  The text contains informal words:\n",
        "\n",
        "    * \"یه\" instead of \"یک\"\n",
        "\n",
        "    * \"محاوره‌ اس\" instead of \"محاوره‌ای است\"\n",
        "\n",
        "    * \"واسه\"  instead of \"برای\"\n",
        "\n",
        "4. Normalizing the text: The function identifies informal words and converts them into formal equivalents.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "T-7CJdkiOFh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "\n",
        "informal_normalizer = hazm.InformalNormalizer()\n",
        "txt = \"این یه متن محاوره‌ اس واسه آموزشه\"\n",
        "res = informal_normalizer.normalize(txt)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvaETT1-u553",
        "outputId": "4cb80d46-e362-4ccf-bf6e-f931fc6aff42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['این'], ['یک', 'یه'], ['متن'], ['محاوره'], ['اس'], ['برای', 'واسه'], ['آموزشه است', 'آموزش است', 'آموزشه']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WbJ0Wda_PyvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing libraries:\n",
        "\n",
        "  `stopwords_list`: A Hazm function providing a list of Persian stopwords (e.g., \"و\", \"به\", \"در\", etc.).\n",
        "\n",
        "  `string.punctuation`: A set of punctuation marks (e.g., `!`, `?`, `.`, etc.).\n",
        "\n",
        "2. Initializing objects for preprocessing:\n",
        "\n",
        "  `normalizer`: Normalizes words (e.g., unifying forms like \"می‌خواهیم\").\n",
        "\n",
        "  `stopwords`: A list of stopwords to filter out from the text.\n",
        "\n",
        "  `punc`: A collection of punctuation marks to be removed.\n",
        "\n",
        "3. Defining the sample text\n",
        "\n",
        "4. Tokenizing the text\n",
        "\n",
        "5. Normalizing and filtering stopwords and punctuation:\n",
        "\n",
        " For each word in `tok`:\n",
        "\n",
        "    If the word is not in the stopwords list (`x not in stopwords`) and not a punctuation mark (`x not in punc`):\n",
        "\n",
        "      It is normalized using `normalizer.normalize(x)`.\n",
        "\n",
        "      Remaining words are joined with spaces (`\" \".join(...)`).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VuRn4F6-PzR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import stopwords_list\n",
        "import string\n",
        "\n",
        "normalizer = Normalizer()\n",
        "stopwords = stopwords_list()\n",
        "punc = string.punctuation\n",
        "\n",
        "sample = \"این یک متن فارسی است که می‌خواهیم آن را پیش پردازش کرده و کلمات توقف آن را حذف کنیم! و همینطور علائم نقطه گذاری!!\"\n",
        "tok = word_tokenize(sample)\n",
        "s = \" \".join(normalizer.normalize(x) for x in tok if x not in stopwords and x not in punc)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZlRc-9QwKj6",
        "outputId": "1fb224e8-2268-47ae-c847-0f698575ac62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "متن فارسی می‌خواهیم پردازش کلمات توقف حذف همینطور علائم نقطه !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KeLCvMMuQAy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing the Hazm library\n",
        "\n",
        "2. Creating a Lemmatizer object:\n",
        "\n",
        " Its main task is to separate the root and suffix/prefix of words and return the base form.\n",
        "\n",
        "\n",
        "3. Lemmatizing samples"
      ],
      "metadata": {
        "id": "ZoHbaNlrQBVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "sample1 = lemmatizer.lemmatize(\"خواندم\")\n",
        "print(sample1)\n",
        "\n",
        "sample2 = lemmatizer.lemmatize(\"کتابهایی\")\n",
        "print(sample2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9wwhIxSyUlz",
        "outputId": "b2843b3a-be26-4f8a-b653-ac5d2d6c2633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خواند#خوان\n",
            "کتاب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yEBzMMUbQCWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing the Hazm library\n",
        "\n",
        "2.  Creating a Normalizer object:\n",
        "\n",
        "  `Normalizer` is used to normalize text (e.g., replacing half-spaces with regular spaces).\n",
        "\n",
        "3. Creating a Stemmer object:\n",
        "\n",
        "  The `Stemmer` reduces words to their simpler or root form.\n",
        "\n",
        "4. Stemming sample words:\n",
        "\n",
        "  Each word is stemmed, and the result is printed.\n",
        "  \n",
        "  In the last line, \"می خواند\" is normalized before stemming to ensure better processing."
      ],
      "metadata": {
        "id": "12Lu-IKuQC2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "stemmer = Stemmer()\n",
        "print(stemmer.stem(\"کتابها\"))\n",
        "print(stemmer.stem(\"خدایان\"))\n",
        "print(stemmer.stem(\"پاسبان\"))\n",
        "print(stemmer.stem(\"می خواند\"))\n",
        "print(stemmer.stem(normalizer.normalize(\"می خواند\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5TduDZWzUOy",
        "outputId": "c6d59a06-52a8-4e31-dfed-3d4bd8038c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کتاب\n",
            "خدا\n",
            "پاسب\n",
            "می خواند\n",
            "می‌خواند\n"
          ]
        }
      ]
    }
  ]
}