{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRxQg/+b3wP/q2tlf9SBun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baharkarami/Text-Mining-Class/blob/main/spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UjYMCwlUqFUn",
        "outputId": "7a388716-5d4c-421f-8abf-5de7b22b3180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=822f5b7f3605b5d599bc28edc526e7205c8f15f8cc2f5ad547518bf625105283\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4e97DHzHEfA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **`import spacy` :** The spacy library is imported to use its NLP features.\n",
        "\n",
        "2.   **`nlp = spacy.load(\"en_core_web_sm\")` :**The `en_core_web_sm` is a small English language model provided by spaCy.\n",
        "This line loads the model into the `nlp` variable, which will be used for processing text.\n",
        "3.   **`txt = \"this is a test for pre-processing!\"` :** A string of text is defined as `txt`. This text will be analyzed by spaCy.\n",
        "4.   **`doc = nlp(txt)` :** The `nlp` function processes the input text (`txt`) and returns a `Doc` object.\n",
        "The `Doc` object contains the processed representation of the text, including tokens, linguistic annotations, and more.\n",
        "5.   **`for token in doc: print(token.text, token.idx)` :** Each word or meaningful unit (token) in the text is accessible through the `doc` object.'\n",
        "\n",
        "  `token.text`: The actual text of the token.\n",
        "\n",
        "  `token.idx`: The starting index of the token in the original text."
      ],
      "metadata": {
        "id": "kCEGYoW1Egda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"this is a test for pre-processing!\"\n",
        "doc = nlp(txt)\n",
        "for token in doc:\n",
        "  print(token.text, token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztwCsSE1qUVr",
        "outputId": "6b8db538-01b7-4b33-a815-eaf5b3c9936a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this 0\n",
            "is 5\n",
            "a 8\n",
            "test 10\n",
            "for 15\n",
            "pre 19\n",
            "- 22\n",
            "processing 23\n",
            "! 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LvFX15BpGn_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Importing and Loading spaCy\n",
        "\n",
        "2.  Defining the Text\n",
        "\n",
        "3.  Processing the Text\n",
        "\n",
        "4.  Iterating over Tokens: Each word or symbol in the text is extracted as a token.\n",
        "\n",
        "  `token`: Represents an individual word, punctuation, or special character in the text.\n",
        "5.  Iterating over Sentences: The `doc.sents` property breaks the text into sentences.\n",
        "\n",
        "  Each sentence (`sen`) is printed as a whole."
      ],
      "metadata": {
        "id": "ZKYHDFipGofn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"Dr. Smith told students to contact him through LMS or his email address at w.example@email.com. Ms. Karami like to code!\"\n",
        "doc = nlp(txt)\n",
        "for token in doc:\n",
        "  print(token)\n",
        "print(\"------------------------------------\")\n",
        "for sen in doc.sents:\n",
        "  print(sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGHwPKGkvgef",
        "outputId": "db38cee1-7ca6-4a8e-d7d0-6dc99baf258a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Smith\n",
            "told\n",
            "students\n",
            "to\n",
            "contact\n",
            "him\n",
            "through\n",
            "LMS\n",
            "or\n",
            "his\n",
            "email\n",
            "address\n",
            "at\n",
            "w.example@email.com\n",
            ".\n",
            "Ms.\n",
            "Karami\n",
            "like\n",
            "to\n",
            "code\n",
            "!\n",
            "------------------------------------\n",
            "Dr. Smith told students to contact him through LMS or his email address at w.example@email.com.\n",
            "Ms. Karami like to code!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W02iJBvPRUZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Importing and Loading SpaCy\n",
        "\n",
        "2.   Defining the Text\n",
        "\n",
        "3.   Processing the Text\n",
        "\n",
        "4.   Formatting the Output Table Header:\n",
        "\n",
        "  `**print(\"\\t {:<10}\\t {:<10}\\t {:<10}\\t {:<10}\\n\".format(\"alpha\", \"punc\", \"digit\", \"stop\"))**`\n",
        "\n",
        "  A table header is printed with the columns:\n",
        "\n",
        "    `alpha`: Whether the token is alphabetic (is_alpha).\n",
        "\n",
        "    `punc`: Whether the token is punctuation (is_punct).\n",
        "\n",
        "    `digit`: Whether the token is a digit (is_digit).\n",
        "\n",
        "    `stop`: Whether the token is a stop word (is_stop).\n",
        "\n",
        "  **{:<10}**\n",
        "\n",
        "    **`:`** Indicates that formatting instructions follow.\n",
        "\n",
        "    **`<`** Aligns the text or number to the left.\n",
        "\n",
        "    **`10`** Specifies the minimum width of the field (10 characters wide).\n",
        "\n",
        "5.  Iterating over Tokens:\n",
        "\n",
        "  For each token in doc, the following properties are printed:\n",
        "\n",
        "    `token.is_alpha`: `True` if the token consists only of alphabetic characters.\n",
        "\n",
        "    `token.is_punct`: `True` if the token is a punctuation mark.\n",
        "\n",
        "    `token.is_digit`: `True` if the token is a number.\n",
        "\n",
        "    `token.is_stop`: `True` if the token is a stop word (e.g., common words like \"is,\" \"the,\" etc.)."
      ],
      "metadata": {
        "id": "W9jnPTNPRVBt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-Ifxz7cV311"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"this 10 is a test!\"\n",
        "doc = nlp(txt)\n",
        "print(\"\\t {:<10}\\t {:<10}\\t {:<10}\\t {:<10}\\n\".format(\"alpha\", \"punc\", \"digit\", \"stop\"))\n",
        "for token in doc:\n",
        "  print(f\"{token}\\t {(token.is_alpha):<10}\\t {(token.is_punct):<10}\\t {(token.is_digit):<10}\\t {(token.is_stop):<10}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJtFe_xC6rlU",
        "outputId": "82c19b9c-132c-40dc-fb7f-bec23c76af2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t alpha     \t punc      \t digit     \t stop      \n",
            "\n",
            "this\t 1         \t 0         \t 0         \t 1         \n",
            "10\t 0         \t 0         \t 1         \t 0         \n",
            "is\t 1         \t 0         \t 0         \t 1         \n",
            "a\t 1         \t 0         \t 0         \t 1         \n",
            "test\t 1         \t 0         \t 0         \t 0         \n",
            "!\t 0         \t 1         \t 0         \t 0         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pLEH4NjsepZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  The `wikipedia` library is imported to access Wikipedia data through Python.\n",
        "\n",
        "2. The `wikipedia.summary()` function fetches a short summary of the topic \"Iran.\" By default, it retrieves the first few sentences of the Wikipedia page.\n",
        "\n",
        "3.  The `wikipedia.page()` function retrieves the full content of the Wikipedia page for \"Iran.\"\n",
        "\n",
        "  `.content` returns the entire text of the page, excluding tables, images, or references."
      ],
      "metadata": {
        "id": "iKDH-IQLep_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "print(wikipedia.summary(\"Iran\"))\n",
        "print(wikipedia.page(\"Iran\").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wQC63g899Lxg",
        "outputId": "63ae3f87-f1e3-4e2e-b49e-03bc0f8e8fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.\n",
            "A different ISBN is assigned to each separate edition and variation of a publication, but not to a simple reprinting of an existing item. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN, but an unchanged reprint of the hardcover edition keeps the same ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\n",
            "The first version of the ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (any 9-digit SBN can be converted to a 10-digit ISBN by prefixing it with a zero).\n",
            "Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns ISBNs to such books on its own initiative.\n",
            "A separate identifier code of a similar kind, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\n",
            "\n",
            "\n",
            "The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.\n",
            "A different ISBN is assigned to each separate edition and variation of a publication, but not to a simple reprinting of an existing item. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN, but an unchanged reprint of the hardcover edition keeps the same ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\n",
            "The first version of the ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (any 9-digit SBN can be converted to a 10-digit ISBN by prefixing it with a zero).\n",
            "Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns ISBNs to such books on its own initiative.\n",
            "A separate identifier code of a similar kind, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\n",
            "\n",
            "\n",
            "== History ==\n",
            "The Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. In 1965, British bookseller and stationers WHSmith announced plans to implement a standard numbering system for its books. They hired consultants to work on their behalf, and the system was devised by Gordon Foster, emeritus professor of statistics at Trinity College Dublin. The International Organization for Standardization (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the \"Father of the ISBN\") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).\n",
            "The 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.\n",
            "\n",
            "An SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), and it cost US$5.95.\n",
            "Since 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with \"Bookland\" European Article Numbers, which have 13 digits. Since 2016, ISBNs have also been used to identify mobile games by China's Administration of Press and Publication.\n",
            "The United States, with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the Republic of Korea (329,582), Germany (284,000), China (263,066), the UK (188,553) and Indonesia (144,793). Lifetime ISBNs registered in the United States are over 39 million as of 2020.\n",
            "\n",
            "\n",
            "== Overview ==\n",
            "A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book must each have a different ISBN assigned to it.: 12  The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).\n",
            "Section 5 of the International ISBN Agency's official user manual: 11  describes the structure of the 13-digit ISBN, as follows:\n",
            "\n",
            "for a 13-digit ISBN, a prefix element – a GS1 prefix: so far 978 or 979 have been made available by GS1,\n",
            "the registration group element (language-sharing country group, individual country or territory),\n",
            "the registrant element,\n",
            "the publication element, and\n",
            "a checksum character or check digit.\n",
            "A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.\n",
            "\n",
            "\n",
            "=== Issuing process ===\n",
            "ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.\n",
            "A full directory of ISBN agencies is available on the International ISBN Agency website. A list for a few countries is given below:\n",
            "\n",
            "Australia – Thorpe-Bowker\n",
            "Brazil – The National Library of Brazil; (Up to 28 February 2020)\n",
            "Brazil – Câmara Brasileira do Livro (From 1 March 2020)\n",
            "Canada – English Library and Archives Canada, a government agency; French Bibliothèque et Archives nationales du Québec;\n",
            "Colombia – Cámara Colombiana del Libro, an NGO\n",
            "Hong Kong – Books Registration Office (BRO), under the Hong Kong Public Libraries\n",
            "Iceland – Landsbókasafn (National and University Library of Iceland)\n",
            "India – The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development\n",
            "Israel – The Israel Center for Libraries\n",
            "Italy – EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association)\n",
            "Kenya – National Library of Kenya\n",
            "Latvia - Latvian ISBN Agency\n",
            "Lebanon – Lebanese ISBN Agency\n",
            "Maldives – The National Bureau of Classification (NBC)\n",
            "Malta – The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb)\n",
            "Morocco – The National Library of Morocco\n",
            "New Zealand – The National Library of New Zealand\n",
            "Nigeria – National Library of Nigeria\n",
            "Pakistan – National Library of Pakistan\n",
            "Philippines – National Library of the Philippines\n",
            "South Africa – National Library of South Africa\n",
            "Spain – Spanish ISBN Agency – Agencia del ISBN\n",
            "Turkey – General Directorate of Libraries and Publications, a branch of the Ministry of Culture\n",
            "United Kingdom and Republic of Ireland – Nielsen Book Services Ltd, part of NIQ\n",
            "United States – R. R. Bowker\n",
            "\n",
            "\n",
            "=== Registration group element ===\n",
            "\n",
            "The ISBN registration group element is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979),: 11  and can be separated between hyphens, such as \"978-1-...\". Registration groups have primarily been allocated within the 978 prefix element. The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0–5, 600–631, 65, 7, 80–94, 950–989, 9910–9989, and 99901–99993. Books published in rare languages typically have longer group elements.\n",
            "Within the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.\n",
            "The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN.\n",
            "\n",
            "\n",
            "=== Registrant element ===\n",
            "The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.\n",
            "The International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the Global Register of Publishers. This database is freely searchable over the internet.\n",
            "Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.\n",
            "By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.\n",
            "\n",
            "\n",
            "==== English-language pattern ====\n",
            "English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:\n",
            "\n",
            "\n",
            "== Check digits ==\n",
            "A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain the letter 'X'.\n",
            "\n",
            "\n",
            "=== ISBN-10 check digits ===\n",
            "According to the 2001 edition of the International ISBN Agency's official user manual, the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if xi is the ith digit, then x10 must be chosen such that:\n",
            "\n",
            "For example, for an ISBN-10 of 0-306-40615-2:\n",
            "\n",
            "Formally, using modular arithmetic, this is rendered\n",
            "\n",
            "It is also true for ISBN-10s that the sum of all ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:\n",
            "\n",
            "Formally, this is rendered\n",
            "\n",
            "The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits (these proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number). The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN—the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.\n",
            "In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).\n",
            "\n",
            "\n",
            "=== ISBN-10 check digit calculation ===\n",
            "Each of the first nine digits of the 10-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.\n",
            "For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:\n",
            "\n",
            "Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11)—this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of \n",
            "  \n",
            "    \n",
            "      \n",
            "        \n",
            "          x\n",
            "          \n",
            "            10\n",
            "          \n",
            "        \n",
            "      \n",
            "    \n",
            "    {\\displaystyle x_{10}}\n",
            "  \n",
            " required to satisfy this condition is 10, then an 'X' should be used.\n",
            "Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11 − 0 = 11, which is invalid. (Strictly speaking, the first \"modulo 11\" is not needed, but it may be considered to simplify the calculation.)\n",
            "For example, the check digit for the ISBN of 0-306-40615-? is calculated as follows:\n",
            "\n",
            "Thus the check digit is 2.\n",
            "It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:\n",
            "\n",
            "The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.\n",
            "\n",
            "\n",
            "=== ISBN-13 check digit calculation ===\n",
            "Appendix 1 of the International ISBN Agency's official user manual: 33  describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both.\n",
            "Formally, using modular arithmetic, this is rendered:\n",
            "\n",
            "The calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results.\n",
            "For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:\n",
            "\n",
            "s = 9×1 + 7×3 + 8×1 + 0×3 + 3×1 + 0×3 + 6×1 + 4×3 + 0×1 + 6×3 + 1×1 + 5×3\n",
            "  =   9 +  21 +   8 +   0 +   3 +   0 +   6 +  12 +   0 +  18 +   1 +  15\n",
            "  = 93\n",
            "93 / 10 = 9 remainder 3\n",
            "10 –  3 = 7\n",
            "\n",
            "Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.\n",
            "In general, the ISBN check digit is calculated as follows.\n",
            "Let\n",
            "\n",
            "Then\n",
            "\n",
            "This check system—similar to the UPC check digit formula—does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3 × 6 + 1 × 1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3 × 1 + 1 × 6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit.\n",
            "Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).\n",
            "\n",
            "\n",
            "=== ISBN-10 to ISBN-13 conversion ===\n",
            "A 10-digit ISBN is converted to a 13-digit ISBN by prepending \"978\" to the  ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.\n",
            "\n",
            "\n",
            "=== Errors in usage ===\n",
            "Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0-590-76484-5 is shared by two books—Ninja gaiden: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.\n",
            "Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\". The International Union Library Catalog (a.k.a., WorldCat OCLC—Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library.\n",
            "\n",
            "\n",
            "=== eISBN ===\n",
            "\n",
            "Only the term \"ISBN\" should be used; the terms \"eISBN\" and \"e-ISBN\" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic \"eISBN\" which encompasses all the e-book formats for a title.\n",
            "\n",
            "\n",
            "== EAN format used in barcodes, and upgrading ==\n",
            "The barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number \"978\", the Bookland \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1× and 3× weighting on alternating digits).\n",
            "Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.\n",
            "Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.\n",
            "Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN in North America.\n",
            "\n",
            "\n",
            "== See also ==\n",
            "ASIN (Amazon Standard Identification Number)\n",
            "BICI (Book Item and Component Identifier)\n",
            "Book sources search – a Wikipedia resource that allows search by ISBNs\n",
            "CODEN (serial publication identifier currently used by libraries; replaced by the ISSN for new works)\n",
            "DOI (Digital Object Identifier)\n",
            "ESTC (English Short Title Catalogue)\n",
            "ISAN (International Standard Audiovisual Number)\n",
            "ISRC (International Standard Recording Code)\n",
            "ISTC (International Standard Text Code)\n",
            "ISWC (International Standard Musical Work Code)\n",
            "ISSN (International Standard Serial Number)\n",
            "ISWN (International Standard Wine Number)\n",
            "LCCN (Library of Congress Control Number)\n",
            "License number (East German books) (Book identification system used between 1951 and 1990 in the former GDR)\n",
            "List of group-0 ISBN publisher codes\n",
            "List of group-1 ISBN publisher codes\n",
            "List of ISBN registration groups\n",
            "SICI (Serial Item and Contribution Identifier)\n",
            "VD 16 (Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts, \"Bibliography of Books Printed in the German Speaking Countries of the Sixteenth Century\")\n",
            "VD 17 (Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts, \"Bibliography of Books Printed in the German Speaking Countries of the Seventeenth Century\")\n",
            "\n",
            "\n",
            "== Explanatory notes ==\n",
            "\n",
            "\n",
            "== References ==\n",
            "\n",
            "\n",
            "== External links ==\n",
            "\n",
            "ISO 2108:2017 – International Standard Book Number (ISBN)\n",
            "International ISBN Agency – coordinates and supervises the worldwide use of the ISBN system\n",
            "Numerical List of Group Identifiers – List of language/region prefixes\n",
            "Free conversion tool: ISBN-10 to ISBN-13 and ISBN-13 to ISBN-10 from the ISBN agency. Also shows correct hyphenation and verifies if ISBNs are valid or not.\n",
            "\"Guidelines for the Implementation of 13-Digit ISBNs\" (PDF). Archived from the original (PDF) on 12 September 2004.\n",
            "RFC 3187 – Using International Standard Book Numbers as Uniform Resource Names (URN)\n",
            "Worldwide Auto-Converter at Library of Congress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "woXARJLyf_aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Import Libraries\n",
        "\n",
        "\n",
        "2.  The `wikipedia.summary()` function is used to fetch the summary of the \"Armageddon\" page. The result is stored in the `txt` variable.\n",
        "\n",
        "\n",
        "3.  The `nlp` pipeline processes the fetched text (`txt`) and returns a SpaCy Doc object (`doc`) which contains all the tokens (words and punctuation) in the text.\n",
        "\n",
        "\n",
        "4.  Get Stop Words:  retrieves the list of stop words from spaCy's built-in `STOP_WORDS` collection for the *English language*. Stop words are common words that are usually filtered out in natural language processing (e.g., \"is,\" \"the,\" \"and\").\n",
        "\n",
        "\n",
        "5.  Filter Out Stop Words: `[tok for tok in doc if tok.text not in stop_words]` filters out tokens from `doc` that are stop words. The filtered tokens are stored in `word_1` and printed.\n",
        "\n",
        "6.  Filter Out Punctuation:  `[tok for tok in doc if not tok.is_punct]` filters out tokens that are punctuation marks (using `token.is_punct`) and stores the non-punctuation tokens in `word_2`, which is then printed."
      ],
      "metadata": {
        "id": "OKGBkW8qgAS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import wikipedia\n",
        "\n",
        "txt = wikipedia.summary(\"armageddon\")\n",
        "doc = nlp(txt)\n",
        "\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "word_1 = [tok for tok in doc if tok.text not in stop_words]\n",
        "print(word_1)\n",
        "\n",
        "word_2 = [tok for tok in doc if not tok.is_punct]\n",
        "print(word_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwpQ4CEW96nz",
        "outputId": "b589de84-6fb6-4905-fdab-1e9df08f7099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[According, Book, Revelation, New, Testament, Christian, Bible, ,, Armageddon, (, AR, -, mə, -, GHED, -, ən, ;, Ancient, Greek, :, Ἁρμαγεδών, ,, romanized, :, Harmagedṓn, ;, Late, Latin, :, Armagedōn, ;, Hebrew, :, הַר, מְגִדּוֹ, ,, romanized, :, Har, Məgīddō, ), prophesied, location, gathering, armies, battle, end, times, ,, variously, interpreted, literal, symbolic, location, ., The, term, generic, sense, refer, end, -, -, -, world, scenario, ., In, Islamic, theology, ,, Armageddon, mentioned, Hadith, Greatest, Armageddon, Al, -, Malhama, Al, -, Kubra, (, great, battle, ), ., \n",
            ", The, \", mount, \", Megiddo, northern, Israel, actually, mountain, ,, tell, (, mound, hill, created, generations, people, living, rebuilding, spot, ), ancient, forts, built, guard, Via, Maris, ,, ancient, trade, route, linking, Egypt, northern, empires, Syria, ,, Anatolia, Mesopotamia, ., Megiddo, location, ancient, battles, ,, including, 15th, century, BC, 609, BC, ., The, nearby, modern, Megiddo, kibbutz, Kishon, River, area, .]\n",
            "[According, to, the, Book, of, Revelation, in, the, New, Testament, of, the, Christian, Bible, Armageddon, AR, mə, GHED, ən, Ancient, Greek, Ἁρμαγεδών, romanized, Harmagedṓn, Late, Latin, Armagedōn, from, Hebrew, הַר, מְגִדּוֹ, romanized, Har, Məgīddō, is, the, prophesied, location, of, a, gathering, of, armies, for, a, battle, during, the, end, times, which, is, variously, interpreted, as, either, a, literal, or, a, symbolic, location, The, term, is, also, used, in, a, generic, sense, to, refer, to, any, end, of, the, world, scenario, In, Islamic, theology, Armageddon, is, also, mentioned, in, Hadith, as, the, Greatest, Armageddon, or, Al, Malhama, Al, Kubra, the, great, battle, \n",
            ", The, mount, of, Megiddo, in, northern, Israel, is, not, actually, a, mountain, but, a, tell, a, mound, or, hill, created, by, many, generations, of, people, living, and, rebuilding, at, the, same, spot, on, which, ancient, forts, were, built, to, guard, the, Via, Maris, an, ancient, trade, route, linking, Egypt, with, the, northern, empires, of, Syria, Anatolia, and, Mesopotamia, Megiddo, was, the, location, of, various, ancient, battles, including, one, in, the, 15th, century, BC, and, one, in, 609, BC, The, nearby, modern, Megiddo, is, a, kibbutz, in, the, Kishon, River, area]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U61fp4MWumna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import spaCy and Load Model\n",
        "2. Fetche the summary of the Wikipedia page for \"Armageddon\" using the `wikipedia.summary()` function.\n",
        "3. The text (`txt`) is passed through the `nlp` pipeline, which processes the text and returns a spaCy `Doc` object. This object contains all the tokens (words and punctuation) in the text.\n",
        "4. Filter Punctuation and Stop Words:\n",
        "\n",
        "  A list comprehension iterates through each token in the `doc`.\n",
        "\n",
        "  `tok.is_punct` checks if the token is punctuation.\n",
        "\n",
        "  `tok.is_stop` checks if the token is a stop word (common words like \"the\", \"and\", \"in\", etc.).\n",
        "\n",
        "  The list includes only tokens that are neither punctuation nor stop words.\n"
      ],
      "metadata": {
        "id": "TAJGDSbtunLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = wikipedia.summary(\"armageddon\")\n",
        "\n",
        "doc = nlp(txt)\n",
        "\n",
        "word = [tok for tok in doc if not tok.is_punct and not tok.is_stop]\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKCZaLux_V2l",
        "outputId": "5cfbfde8-3198-416d-a6ac-cc5b8aa6ad03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[According, Book, Revelation, New, Testament, Christian, Bible, Armageddon, AR, mə, GHED, ən, Ancient, Greek, Ἁρμαγεδών, romanized, Harmagedṓn, Late, Latin, Armagedōn, Hebrew, הַר, מְגִדּוֹ, romanized, Har, Məgīddō, prophesied, location, gathering, armies, battle, end, times, variously, interpreted, literal, symbolic, location, term, generic, sense, refer, end, world, scenario, Islamic, theology, Armageddon, mentioned, Hadith, Greatest, Armageddon, Al, Malhama, Al, Kubra, great, battle, \n",
            ", mount, Megiddo, northern, Israel, actually, mountain, tell, mound, hill, created, generations, people, living, rebuilding, spot, ancient, forts, built, guard, Maris, ancient, trade, route, linking, Egypt, northern, empires, Syria, Anatolia, Mesopotamia, Megiddo, location, ancient, battles, including, 15th, century, BC, 609, BC, nearby, modern, Megiddo, kibbutz, Kishon, River, area]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ICRy2Qt58PFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import spaCy and Load Model\n",
        "\n",
        "2. Text Processing: The text (`txt`) is defined and passed through the SpaCy `nlp` pipeline.\n",
        "`doc` is a spaCy Doc object containing all the tokens (words and punctuation) from the input text.\n",
        "\n",
        "3. Lemmatization:\n",
        "  * The `for` loop iterates over each token (word or punctuation) in the `doc` object.\n",
        "\n",
        "  * `token.lemma_` returns the **lemma** (base form) of the token. For example:\n",
        "    * The lemma of \"running\" is \"run\".\n",
        "    * The lemma of \"saw\" is \"see\".\n",
        "  * Then the lemmas of all tokens are printed."
      ],
      "metadata": {
        "id": "-s9Ld9yD8P3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"I saw a grop of cats, running to the park! When I met the park next day, I saw none of them.\"\n",
        "doc = nlp(txt)\n",
        "\n",
        "for token in doc:\n",
        "  print(token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBtz5E2WA85e",
        "outputId": "5e835cab-6fdb-4f64-b7a5-9d3a054bc64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "see\n",
            "a\n",
            "grop\n",
            "of\n",
            "cat\n",
            ",\n",
            "run\n",
            "to\n",
            "the\n",
            "park\n",
            "!\n",
            "when\n",
            "I\n",
            "meet\n",
            "the\n",
            "park\n",
            "next\n",
            "day\n",
            ",\n",
            "I\n",
            "see\n",
            "none\n",
            "of\n",
            "they\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XjwFf6ZdKhQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load spaCy and Initialize NLP Model\n",
        "\n",
        "2. Define and Process Text\n",
        "\n",
        "3. Extract Lemmas:\n",
        "  * A list comprehension is used to extract the lemma of each token in `doc` (`token.lemma_`).\n",
        "\n",
        "  * The extracted lemmas are joined into a single string using `\" \".join()`.\n",
        "\n",
        "4. Result: The output is a string with each token replaced by its lemma."
      ],
      "metadata": {
        "id": "iDskQv2FKh2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"I saw a grop of cats, running to the park! When I met the park next day, I saw none of them.\"\n",
        "doc = nlp(txt)\n",
        "\n",
        "print(\" \".join([token.lemma_ for token in doc]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3xMXlYSBqg8",
        "outputId": "35717d1f-9481-411e-d8a2-cc8fb3462d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I see a grop of cat , run to the park ! when I meet the park next day , I see none of they .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JP04RA-nLsEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load spaCy and Initialize NLP Model\n",
        "\n",
        "2. Define and Process Text\n",
        "\n",
        "3. Format Output:\n",
        "  \n",
        "  A list comprehension iterates over each token in the `doc`.\n",
        "\n",
        "  For each token:\n",
        "    * `tok` represents the token text.\n",
        "\n",
        "    * `tok.pos_` represents the part-of-speech tag (e.g., noun, verb).\n",
        "\n",
        "  `f\"{tok}[{tok.pos_}]\"` formats the token and its POS tag as `token[POS]`.\n",
        "\n",
        "  The result is a list of strings.\n",
        "\n",
        "**Output**: A list of tokens with their respective POS tags in the format `token[POS]`."
      ],
      "metadata": {
        "id": "W0IUTWZzLspx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"I saw a grop of cats, running to the park! When I met the park next day, I saw none of them.\"\n",
        "doc = nlp(txt)\n",
        "\n",
        "print([f\"{tok}[{tok.pos_}]\"  for tok in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkDi2Z8AB7iU",
        "outputId": "4fdbd39f-4f37-4436-9420-067bbc61d83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I[PRON]', 'saw[VERB]', 'a[DET]', 'grop[NOUN]', 'of[ADP]', 'cats[NOUN]', ',[PUNCT]', 'running[VERB]', 'to[ADP]', 'the[DET]', 'park[NOUN]', '![PUNCT]', 'When[SCONJ]', 'I[PRON]', 'met[VERB]', 'the[DET]', 'park[NOUN]', 'next[ADJ]', 'day[NOUN]', ',[PUNCT]', 'I[PRON]', 'saw[VERB]', 'none[NOUN]', 'of[ADP]', 'them[PRON]', '.[PUNCT]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "43wsnJFMO7HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load spaCy and Initialize NLP Model\n",
        "\n",
        "2. Define and Process Text\n",
        "\n",
        "3. Extract and Explain Named Entities:\n",
        "\n",
        "  `doc.ents` contains all named entities detected in the text.\n",
        "\n",
        "  For each entity:\n",
        "\n",
        "  `ent.text`: The actual text of the entity.\n",
        "\n",
        "  `ent.label_`: The named entity label (e.g., PERSON, GPE, ORG).\n",
        "\n",
        "  `spacy.explain(ent.label_)`: Provides a description of the label.\n",
        "\n",
        "4. **Output**: The output displays the entity, its label, and an explanation of the label.\n",
        "\n"
      ],
      "metadata": {
        "id": "PxhPwiEfO7vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "txt = \"\"\"Iran is a big country in MiddleEast. Leader of Iran is Ayatollah Khomeini.\n",
        "The president of Iran is Pezeshkian. In U.S. President Obama.\"\"\"\n",
        "doc = nlp(txt)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(f\"{ent.text}: [{ent.label_}] -> {spacy.explain(ent.label_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U49_1KFnCPeG",
        "outputId": "52d9c211-e47e-4e1f-e7fb-af8b3b02c0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iran: [GPE] -> Countries, cities, states\n",
            "MiddleEast: [ORG] -> Companies, agencies, institutions, etc.\n",
            "Iran: [GPE] -> Countries, cities, states\n",
            "Ayatollah Khomeini: [PERSON] -> People, including fictional\n",
            "Iran: [GPE] -> Countries, cities, states\n",
            "U.S.: [GPE] -> Countries, cities, states\n",
            "Obama: [GPE] -> Countries, cities, states\n"
          ]
        }
      ]
    }
  ]
}